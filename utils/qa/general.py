from typing import Union

from langchain_core.runnables import RunnableSequence
from langchain_anthropic import ChatAnthropic
from langchain_openai import ChatOpenAI

from langfuse.callback import CallbackHandler

from utils.qa.prompt_template import (
    GeneralResponseTemplate,
    GeneralStructureResponseTemplate,
)
from utils.parsers.output_parse import StructureJsonOutputParser


class ResponseGenerator:
    __slots__ = ["lc_client", "lf_handler", "chat_mode"]

    def __init__(
        self,
        lc_client: Union[ChatAnthropic, ChatOpenAI],
        lf_handler: CallbackHandler,
        chat_mode: bool,
    ):
        """
        A class responsible for generating a response to a user query based on
        the query and context information retrieved from external sources

        Attributes
        ----------
        lc_client: Union[ChatAnthropic, ChatOpenAI]
            The LangChain client that will be used to interact with LLM for
            generating response
        lf_handler: langfuse.callback.CallbackHandler
            Handler for logging and tracing LLM interactions via Langfuse
        chat_mode: bool
            A flag indicating whether chat mode is enabled
        """
        # Init langchain LLM client, langfuse handler and define chat mode parameter
        self.lc_client = lc_client
        self.lf_handler = lf_handler
        self.chat_mode = chat_mode

    def _init_chain_of_generation(self) -> RunnableSequence:
        """
        Initializes the chain of components used for generating responses,
        incorporating prompt template based on chat mode

        Returns
        -------
        RunnableSequence
            A combined chain of the prompt template and LangChain client
            responsible for generating responses
        """
        if self.chat_mode:
            return (GeneralResponseTemplate().formulate() | self.lc_client).with_config(
                {"run_name": "GeneralResponse"}
            )
        else:
            return (
                GeneralStructureResponseTemplate().formulate()
                | self.lc_client
                | StructureJsonOutputParser()
            ).with_config({"run_name": "GeneralStructureResponse"})

    def generate(self, query: str, retrieved_info: str) -> str:
        """
        Generates a response to the user's query using the provided context
        information

        Parameters
        ----------
        query: str
            The user query that requires an answer
        retrieved_info: str
            The context or additional information retrieved to assist in answering
            the user's query

        Returns
        -------
        str
            The response generated by the LangChain client, answering the user's
            query
        """
        # Determine chain of response generation
        runnable_chain = self._init_chain_of_generation()

        # Answer user query with chat history
        response = runnable_chain.invoke(
            {"query": query, "relevant_info": retrieved_info},
            config={"callbacks": [self.lf_handler]},
        ).content

        return response
